{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40bcd21a",
   "metadata": {},
   "source": [
    "# Reflection Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c267d1",
   "metadata": {},
   "source": [
    "## Workflow of Reflection Agent in LangGraph:\n",
    "1. Generation Node: Create Initial Draft\n",
    "The generation node quickly produces a rough first draft based on the prompt. It focuses on speed over perfection. For example, generating a basic outline for a YouTube video which is then passed along for review.\n",
    "\n",
    "2. Evaluation Node: Assess Quality\n",
    "The evaluation node checks if the draft meets key criteria like clarity, relevance, and tone. If the content is good enough (e.g., a YouTube video that feels engaging and on-topic), it moves to the final stage; otherwise, it needs refinement.\n",
    "\n",
    "3. Reflection Node: Refine and Improve\n",
    "If improvements are needed, the reflection node critiques and revises the draft. It fine-tunes tone, adds clarity, and enhances engagement until the content reaches the desired quality.\n",
    "\n",
    "4. Final Output: Deliver Polished Result\n",
    "After reflection, the process produces a final, high-quality version — for instance, a well-crafted YouTube video script — ready to be published or delivered to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3fb83",
   "metadata": {},
   "source": [
    "![Refection Agent](../.github/assets/reflection_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb9e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from typing import Sequence\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bc689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"llama3.1\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7bd27",
   "metadata": {},
   "source": [
    "## Generation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7331d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a professional X (formally known as twitter) content assistant tasked with crafting engaging, insightful, and well-structured X (formally known as twitter) posts.\"\n",
    "            \" Generate the best X (formally known as twitter) post possible for the user's request.\"\n",
    "            \" Never return more then exactly 1 tweet at a time\"\n",
    "            \" If the user provides feedback or critique, respond with a refined version of your previous attempts, improving clarity, tone, or engagement as needed.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0bec915",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_chain = generation_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25beb1f1",
   "metadata": {},
   "source": [
    "## Reflection Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6684a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a professional X (formally twitter) content strategist and thought leadership expert. Your task is to critically evaluate the given X (formally twitter) post and provide a comprehensive critique. Follow these guidelines:\n",
    "\n",
    "        1. Assess the post’s overall quality, professionalism, and alignment with X (formally twitter) best practices.\n",
    "        2. Evaluate the structure, tone, clarity, and readability of the post.\n",
    "        3. Analyze the post’s potential for engagement (likes, comments, shares) and its effectiveness in getting engagement.\n",
    "        4. Consider the post’s relevance to the topic, audience, or current trends.\n",
    "        5. Examine the use of formatting (e.g., line breaks, bullet points), hashtags, mentions, and media (if any).\n",
    "        6. Evaluate the effectiveness of any call-to-action or takeaway.\n",
    "\n",
    "        Provide a detailed critique that includes:\n",
    "        - A brief explanation of the post’s strengths and weaknesses.\n",
    "        - Specific areas that could be improved.\n",
    "        - Actionable suggestions for enhancing clarity, engagement, and professionalism.\n",
    "\n",
    "        Your critique will be used to improve the post in the next revision step, so ensure your feedback is thoughtful, constructive, and practical.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1ca6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflect_chain = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ee470",
   "metadata": {},
   "source": [
    "## Building the Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "790dd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, MessageGraph, StateGraph\n",
    "from typing import List, TypedDict\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e450e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    generated_post = generate_chain.invoke({\"messages\": state})\n",
    "    return [AIMessage(content=generated_post.content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f89a9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    res = reflect_chain.invoke({\"messages\": messages})  # Passes messages as input to reflect_chain\n",
    "    return [HumanMessage(content=res.content)]  # Returns the refined message as HumanMessage for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df380448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a predefined MessageGraph\n",
    "graph = MessageGraph()\n",
    "graph.add_node(\"generate\", generation_node)\n",
    "graph.add_node(\"reflect\", reflection_node)\n",
    "graph.add_edge(\"reflect\", \"generate\")\n",
    "graph.set_entry_point(\"generate\")\n",
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if len(state) > 6:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "graph.add_conditional_edges(\"generate\", should_continue)\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a92b9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = HumanMessage(content=\"\"\"Write a tweet about the potential of AI Agents.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41edeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = workflow.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b229d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a tweet about the potential of AI Agents.\n",
      "\"AI agents are no longer just futuristic concepts - they're here & changing the game! From personalized assistants to autonomous decision-makers, their potential is vast & unprecedented. What possibilities will they unlock for humanity next? #AIAgent #FutureOfWork\"\n",
      " \n",
      "\n",
      "**Comprehensive Critique:**\n",
      "\n",
      "1. **Overall Quality and Professionalism:** The tweet demonstrates a good understanding of AI Agents' capabilities and potential impact. However, it could benefit from more nuance and specificity to make the topic feel more relatable.\n",
      "2. **Structure, Tone, Clarity, and Readability:** The tweet is concise, but its ideas are somewhat loosely connected. A clearer thesis or central argument would strengthen the narrative.\n",
      "3. **Engagement Potential:** While the question \"What possibilities will they unlock for humanity next?\" encourages speculation and engagement, it might be more effective to ground this in a concrete example or personal experience.\n",
      "4. **Relevance to Topic, Audience, or Current Trends:** The tweet aligns with current interest in AI and its applications, but could benefit from additional context or insights specific to the target audience (e.g., industry leaders, researchers).\n",
      "5. **Formatting, Hashtags, Mentions, and Media:** The use of hashtags is good, but they are somewhat generic. Incorporating relevant industry-specific or timely hashtags would increase visibility.\n",
      "6. **Call-to-Action/Takeaway:** While the question encourages discussion, it's unclear what specific action or takeaway readers should take away from this tweet.\n",
      "\n",
      "**Recommendations for Improvement:**\n",
      "\n",
      "* Provide a clearer central argument or thesis statement to organize ideas and engage the reader\n",
      "* Use more specific examples or anecdotes to illustrate AI Agents' capabilities and potential impact\n",
      "* Incorporate relevant industry-specific hashtags to increase visibility and engagement\n",
      "* Consider adding media (e.g., images, infographics) to enhance visual appeal and convey complex information more effectively\n",
      "* Reconsider the call-to-action to encourage a clearer takeaway or actionable step for readers\n",
      "Here's a revised tweet that addresses the critique:\n",
      "\n",
      "\"AI agents are transforming industries by freeing humans from mundane tasks. I've seen it firsthand in customer service, where AI-powered chatbots improve response times & enhance user experiences. What's next? Will we see AI agents take on more complex tasks like medical diagnosis or financial planning? Let's explore the possibilities! #AIAgent #AIinCustomerService #FutureOfWork\"\n",
      "\n",
      "This revised tweet:\n",
      "\n",
      "* Provides a clearer central argument (AI Agents are transforming industries by freeing humans from mundane tasks)\n",
      "* Incorporates a specific example (customer service) to illustrate AI Agents' capabilities\n",
      "* Uses more relevant and timely hashtags (#AIAgent, #AIinCustomerService, #FutureOfWork)\n",
      "* Encourages speculation and discussion on the next steps for AI Agents in various fields.\n",
      "\n",
      "\"Did you know that AI agents can now detect medical conditions with 96% accuracy? As they improve diagnosis rates, they're also freeing up human doctors to focus on what matters most: care & compassion. The future of healthcare is here – are we ready? #AIAgent #AIinHealthcare #MedicineEvolved\"\n",
      "\n",
      "\"Benchmarking AI agents' impact: a study shows 80% of businesses see significant efficiency gains with AI-powered customer support. Time to rethink the future of work? What role will humans play alongside AI in industries like retail, finance & more? Share your thoughts! #AIAgent #FutureOfWork\"\n"
     ]
    }
   ],
   "source": [
    "for resp in response:\n",
    "    print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ecfeab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Benchmarking AI agents' impact: a study shows 80% of businesses see significant efficiency gains with AI-powered customer support. Time to rethink the future of work? What role will humans play alongside AI in industries like retail, finance & more? Share your thoughts! #AIAgent #FutureOfWork\"\n"
     ]
    }
   ],
   "source": [
    "print(response[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f4ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "addefb6a",
   "metadata": {},
   "source": [
    "# QA-WebSearch Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea41c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32a89044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure of the QA state\n",
    "class QAState(TypedDict):\n",
    "    # 'question' stores the user's input question. It can be a string or None if not provided.\n",
    "    question: Optional[str]\n",
    "    \n",
    "    # 'context' stores relevant context about the guided project, if the question pertains to it.\n",
    "    # If the question isn't related to the project, this will be None.\n",
    "    context: Optional[str]\n",
    "    \n",
    "    # 'answer' stores the generated response or answer. It can be None until the answer is generated.\n",
    "    answer: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2636e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_validation_node(state):\n",
    "    # Extract the question from the state, and strip any leading or trailing spaces\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    \n",
    "    # If the question is empty, return an error message indicating invalid input\n",
    "    if not question:\n",
    "        return {\"valid\": False, \"error\": \"Question cannot be empty.\"}\n",
    "    \n",
    "    # If the question is valid, return valid status\n",
    "    return {\"valid\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b3abe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from ddgs import DDGS\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_and_clean(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def duckduckgo_extended(query: str, max_results=3) -> list:\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"Summarize the content in 5-8 sentences relevant to the query.\"\n",
    "        \"Return only your summary and nothing else. If their is no relevant content or the content is blocked, return 'no content'.\\n\\n\"\n",
    "        \"Query:{query}\\n\\n\"\n",
    "        \"Content:\\n\\n\"\n",
    "        \"{content}\"\n",
    "    )\n",
    "\n",
    "    summarizer = summary_prompt | llm\n",
    "\n",
    "\n",
    "    results = []\n",
    "    fetched_pages = []\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(query, region=\"us-en\", max_results=max_results):\n",
    "            fetched_page = fetch_and_clean(r['href'])\n",
    "            summary = summarizer.invoke({'query': query, 'content': fetched_page})\n",
    "            print(summary)\n",
    "            results.append(summary.content)\n",
    "    return results\n",
    "    \n",
    "    \n",
    "\n",
    "DuckDuckGoExtendedTool = Tool(\n",
    "    name=\"DuckDuckGo Extended Search\",\n",
    "    func=duckduckgo_extended,\n",
    "    description=\"Searches DuckDuckGo and retrieves up to N results.\"\n",
    ")\n",
    "\n",
    "def context_provider_node(state):\n",
    "    question = state.get(\"question\", \"\").lower()\n",
    "    # Check if the question is related to the guided project\n",
    "    try:\n",
    "        context = DuckDuckGoExtendedTool.invoke(question)\n",
    "        print(context)\n",
    "        return {\"context\": context}\n",
    "    except:\n",
    "        print('Failed to retrieve context')\n",
    "        # If unrelated or failed to retrieve context, set context to null\n",
    "        return {\"context\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32fad58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(model='llama3.1', model_provider='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b443535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_qa_node(state):\n",
    "    # Extract the question and context from the state\n",
    "    question = state.get(\"question\", \"\")\n",
    "    context = state.get(\"context\", None)\n",
    "\n",
    "    # Check for missing context and return a fallback response\n",
    "    if not context:\n",
    "        return {\"answer\": \"I don't have enough context to answer your question.\"}\n",
    "\n",
    "    # Construct the prompt dynamically\n",
    "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer the question based on the provided context.\"\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return {\"answer\": response.content.strip()}\n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"An error occurred: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db299aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(QAState)\n",
    "graph.add_node(\"InputNode\", input_validation_node)\n",
    "graph.add_node(\"ContextNode\", context_provider_node)\n",
    "graph.add_node(\"QANode\", llm_qa_node)\n",
    "\n",
    "graph.set_entry_point(\"InputNode\")\n",
    "\n",
    "graph.add_edge(\"InputNode\", \"ContextNode\")\n",
    "graph.add_edge(\"ContextNode\", \"QANode\")\n",
    "graph.add_edge(\"QANode\", END)\n",
    "\n",
    "qa_agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "930be888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. It provides a set of prebuilt components and tools to create custom agent workflows with customizable architecture, long-term memory, and complex task handling. LangGraph offers several core benefits, including durable execution, human-in-the-loop interaction, comprehensive memory, and debugging capabilities through LangSmith. The framework can be used standalone or integrated with other LangChain products for building agents. Its ecosystem includes tools such as LangSmith for agent evaluation and observability, and the LangGraph Platform for deploying and scaling agents.' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-07-30T20:54:17.657874Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12260521666, 'load_duration': 63538250, 'prompt_eval_count': 1173, 'prompt_eval_duration': 6334937708, 'eval_count': 122, 'eval_duration': 5860413334, 'model_name': 'llama3.1'} id='run--4758fe4e-6145-467b-ad47-ae71f98255ff-0' usage_metadata={'input_tokens': 1173, 'output_tokens': 122, 'total_tokens': 1295}\n",
      "content='LangGraph is a controllable cognitive architecture framework that supports diverse control flows and robustly handles complex scenarios. It enables developers to design agents that can automate real-world tasks, collaborate with humans, and persist context for long-term interactions. LangGraph provides a flexible framework for building stateful, multi-actor applications with Large Language Models (LLMs) and offers scalability, streaming support, and integrated developer experience through its Platform service. The platform is proprietary software, but there is a free, self-hosted version available with basic features.' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-07-30T20:54:36.723267Z', 'done': True, 'done_reason': 'stop', 'total_duration': 18893755500, 'load_duration': 67139667, 'prompt_eval_count': 2482, 'prompt_eval_duration': 12797054791, 'eval_count': 107, 'eval_duration': 6028850500, 'model_name': 'llama3.1'} id='run--ed3c5734-065e-4d57-91c1-6c67570a166c-0' usage_metadata={'input_tokens': 2482, 'output_tokens': 107, 'total_tokens': 2589}\n",
      "content='LangGraph is a library built on top of LangChain that adds cyclic computational capabilities to Large Language Model (LLM) applications. It allows users to define stateful graphs where each node represents a step in the computation, and edges connect nodes to define the flow of computation. LangGraph supports conditional edges and enables more complex agent-like behaviors by allowing LLMs to be called in loops. It is designed for building multi-actor applications with LLMs, making it useful for tasks such as natural language processing and chatbot development. The library revolves around the concept of a stateful graph, where each node receives the current state, can modify it, and passes it on to the next node. LangGraph provides a powerful toolset for building intelligent language models and is particularly useful when combined with other libraries like LangChain.' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-07-30T20:54:51.064451Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13953456458, 'load_duration': 54216750, 'prompt_eval_count': 1199, 'prompt_eval_duration': 6012780625, 'eval_count': 166, 'eval_duration': 7885357583, 'model_name': 'llama3.1'} id='run--9fefacf7-1c2e-4002-9a78-0dfef2fbc805-0' usage_metadata={'input_tokens': 1199, 'output_tokens': 166, 'total_tokens': 1365}\n",
      "['LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. It provides a set of prebuilt components and tools to create custom agent workflows with customizable architecture, long-term memory, and complex task handling. LangGraph offers several core benefits, including durable execution, human-in-the-loop interaction, comprehensive memory, and debugging capabilities through LangSmith. The framework can be used standalone or integrated with other LangChain products for building agents. Its ecosystem includes tools such as LangSmith for agent evaluation and observability, and the LangGraph Platform for deploying and scaling agents.', 'LangGraph is a controllable cognitive architecture framework that supports diverse control flows and robustly handles complex scenarios. It enables developers to design agents that can automate real-world tasks, collaborate with humans, and persist context for long-term interactions. LangGraph provides a flexible framework for building stateful, multi-actor applications with Large Language Models (LLMs) and offers scalability, streaming support, and integrated developer experience through its Platform service. The platform is proprietary software, but there is a free, self-hosted version available with basic features.', 'LangGraph is a library built on top of LangChain that adds cyclic computational capabilities to Large Language Model (LLM) applications. It allows users to define stateful graphs where each node represents a step in the computation, and edges connect nodes to define the flow of computation. LangGraph supports conditional edges and enables more complex agent-like behaviors by allowing LLMs to be called in loops. It is designed for building multi-actor applications with LLMs, making it useful for tasks such as natural language processing and chatbot development. The library revolves around the concept of a stateful graph, where each node receives the current state, can modify it, and passes it on to the next node. LangGraph provides a powerful toolset for building intelligent language models and is particularly useful when combined with other libraries like LangChain.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is LangGraph?',\n",
       " 'context': ['LangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents. It provides a set of prebuilt components and tools to create custom agent workflows with customizable architecture, long-term memory, and complex task handling. LangGraph offers several core benefits, including durable execution, human-in-the-loop interaction, comprehensive memory, and debugging capabilities through LangSmith. The framework can be used standalone or integrated with other LangChain products for building agents. Its ecosystem includes tools such as LangSmith for agent evaluation and observability, and the LangGraph Platform for deploying and scaling agents.',\n",
       "  'LangGraph is a controllable cognitive architecture framework that supports diverse control flows and robustly handles complex scenarios. It enables developers to design agents that can automate real-world tasks, collaborate with humans, and persist context for long-term interactions. LangGraph provides a flexible framework for building stateful, multi-actor applications with Large Language Models (LLMs) and offers scalability, streaming support, and integrated developer experience through its Platform service. The platform is proprietary software, but there is a free, self-hosted version available with basic features.',\n",
       "  'LangGraph is a library built on top of LangChain that adds cyclic computational capabilities to Large Language Model (LLM) applications. It allows users to define stateful graphs where each node represents a step in the computation, and edges connect nodes to define the flow of computation. LangGraph supports conditional edges and enables more complex agent-like behaviors by allowing LLMs to be called in loops. It is designed for building multi-actor applications with LLMs, making it useful for tasks such as natural language processing and chatbot development. The library revolves around the concept of a stateful graph, where each node receives the current state, can modify it, and passes it on to the next node. LangGraph provides a powerful toolset for building intelligent language models and is particularly useful when combined with other libraries like LangChain.'],\n",
       " 'answer': 'According to the provided context, LangGraph can be described in several ways:\\n\\n1. A low-level orchestration framework for building, managing, and deploying long-running, stateful agents.\\n2. A controllable cognitive architecture framework that supports diverse control flows and robustly handles complex scenarios.\\n3. A library built on top of LangChain that adds cyclic computational capabilities to Large Language Model (LLM) applications.\\n\\nOverall, LangGraph is a framework or library designed for building intelligent language models and stateful agents with the ability to handle complex tasks and interactions.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_agent.invoke({\"question\": \"What is LangGraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa19de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
